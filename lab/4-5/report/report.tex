\documentclass[a4paper,10pt]{article}
\usepackage{fullpage}
\usepackage{times}
\usepackage{listings}
\usepackage{xparse}
\usepackage{hyperref}
\usepackage[a4paper, margin=1in]{geometry}

\usepackage{graphicx}
\graphicspath{ {./images/} }

\usepackage{subfig}
\usepackage[
backend=biber,
style=ieee,
sorting=ynt
]{biblatex}

\addbibresource{references.bib}
 
\usepackage{xcolor}
 
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}
\NewDocumentCommand{\codeword}{v}{%
\texttt{{#1}}%
}




\begin{document}

\title{L41: Lab 3 - The TCP State Machine, Latency, and Bandwidth}
\author{Nicolò Mazzucato \textless{}nm712\textgreater{}}
\date{\today}

\maketitle

\thispagestyle{empty}

\begin{abstract}

   - state machine comparison with the RFC[] one manually setting differnet latencies (differences on the final sequence)
   - analysis of the flow-control and congestion control mechanisms, plotting graphs with the congestion window, and analysing how the tcp state variable change in time.



\end{abstract}

\clearpage

\setcounter{page}{1}

\section{Introduction}

\section{Experimental setup and methodology}

\subsection{Hardware setup}

For the experiment, it is used the BeagleBone Black board with an open-source FreeBSD\cite{mckusick_design_2014} 11.0 operating system’s ARMv7 port. This board has the following characteristics:
\begin{itemize}
    \item 512MB DDR3 RAM
    \item 4GB 8-bit eMMC on-board flash storage
    \item AM335x 1GHz ARM® Cortex-A8 - Single core 32-bit processor\cite{noauthor_am3358_nodate}
    \begin{itemize}
        \item Independent 32KB L1 instructions and data cache
        \item Shared L2 cache (256KB)
        \item 64 bytes cache lines
    \end{itemize}
\end{itemize}

The board mounts an SD card. However, it is not relevant for the test, as the benchmark is in-memory.

\subsection{Methodology}

The benchmark consists of a program able to create two ends of a TCP socket over the loopback interface. The program is statically linked, so we can neglet dynamic linking performance overhead. The program opens a socket using the \codeword{socket(2)} system call, bind it to a specifc port, and listen for new connections. The receiving end similarly opens a socket, and uses \codeword{connect(2)} to connect with the other endpoint at the same port. During the benchmark, a fixed size amount of data is transferred from the sender to the receiver. Afterwards, the \codeword{close(2)} system call initiate the closing phase.

IPFW\cite{IPFW}, the firewall used, provides DUMMYNET, that allows to shape the traffic. In this experiment, IPFW is used to generate two pipes, and sent the latency in each one of those. The pipes are used to filter the traffic, and selectively apply the delay only to the traffic we care abot.
The actors of the communication, in our case, are two threads: one that sends data, and the other that receives it. The dimension of the buffer passed to the \codeword{write(2)} system call are constant to 128KB.
The total amount of data transferred is 16MB for each benchmark run.

The loopback Maximum Transmission Unit (MTU) is manually set to 1,500 bytes before each test, to avoid larger values due to the local nature of the test, and resemble LAN or WAN results.

DTrace is used for most performance measurements. In particualar, we use the following providers:
\begin{itemize}
   \item Function Boundary Tracing (FBT) provider. We are using probes on specific functions. There are also TCP generic ones, portable across different systems, but they do not provide the needed number of parameters to make all the analysis. The output from the tracing of the entire kernel's function is then visualized in Perfetto\cite{noauthor_perfetto_nodate}, to graphically inspect for futher interesting behaviours.
   \item Profile provider. It fires a probe at a given frequency. We use it to aggregate the results of \codeword{stack()}, and see where most of the time is spent. 
\end{itemize}
The verbose benchmark output, containing the IPC loop duration measured from the program, is then used to evaluate the probe-effect caused by DTrace.


All the benchmark runs are executed 11 times for each parameter value, and the first one is dropped. In each graph, it is possible to see the error as a vertical line. However, in almost all cases, this error is negligible, highlighting a controlled benchmarking environment. The main value on the graph is the median of the 10 measurements.


\subsection{Limitations}

During the tracing, several other processes were running, and this might have influenced the results. In particular:
\begin{itemize}
    \item One ssh session
    \item A Jupyter notebook
\end{itemize}



\section{Results and discussion}









\subsection{Probe effect}

\section{Conclusions}

\newpage

\section{References}

\printbibliography

\section{Appendices}


\end{document}
